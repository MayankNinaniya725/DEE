import os
import io
import json
import logging
import tempfile
import shutil
from datetime import datetime
from django.shortcuts import render, redirect
from django.contrib import messages
from django.contrib.auth import logout
from django.conf import settings
from django.http import JsonResponse, FileResponse
from django.utils import timezone
from django.core.exceptions import ValidationError
from celery.result import AsyncResult
import pandas as pd
from openpyxl import load_workbook
from .models import ExtractedData, Vendor, UploadedPDF
from .utils.extractor import extract_pdf_fields
from .utils.config_loader import load_vendor_config
from .tasks import process_pdf_file

# Configure logging
logger = logging.getLogger('extractor')

def create_extraction_excel(excel_path, pdf_obj, extracted_data):
    """Creates a detailed Excel file with multiple sheets for extracted data"""
    pdf_filename = os.path.basename(pdf_obj.file.name)
    
    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
        # Summary sheet
        summary_data = {
            'Information': [
                'File Name', 'Vendor', 'Upload Date', 'Total Fields', 
                'Total Pages', 'Status'
            ],
            'Value': [
                pdf_filename,
                pdf_obj.vendor.name,
                pdf_obj.uploaded_at.strftime("%Y-%m-%d %H:%M:%S"),
                extracted_data.count(),
                len(set(item.page_number for item in extracted_data if item.page_number)),
                'Extraction Complete'
            ]
        }
        pd.DataFrame(summary_data).to_excel(writer, sheet_name='Summary', index=False)
        
        # Main extraction sheet
        main_data = [{
            'Field Type': item.field_key,
            'Extracted Value': item.field_value,
            'Page Number': item.page_number,
            'PDF Location': f'extracted_pdfs/page_{item.page_number}.pdf' if item.page_number else 'N/A',
            'Extracted At': item.created_at.strftime("%Y-%m-%d %H:%M:%S")
        } for item in extracted_data]
        
        if main_data:
            pd.DataFrame(main_data).to_excel(writer, sheet_name='Extracted Data', index=False)
        
        # Key Fields sheet
        key_fields = ['PLATE_NO', 'HEAT_NO', 'TEST_CERT_NO']
        key_data = []
        
        for field in key_fields:
            matches = [item for item in extracted_data if item.field_key == field]
            for match in matches:
                key_data.append({
                    'Field': field,
                    'Value': match.field_value,
                    'Page': match.page_number,
                    'PDF File': f'extracted_pdfs/page_{match.page_number}.pdf',
                    'Status': 'Verified' if match.field_value else 'Not Found'
                })
        
        if key_data:
            pd.DataFrame(key_data).to_excel(writer, sheet_name='Key Fields', index=False)
        
        # Page Summary sheet
        page_data = []
        for page in sorted(set(item.page_number for item in extracted_data if item.page_number)):
            page_fields = [item for item in extracted_data if item.page_number == page]
            key_fields_found = [
                f"{item.field_key}: {item.field_value}"
                for item in page_fields
                if item.field_key in key_fields and item.field_value
            ]
            
            page_data.append({
                'Page Number': page,
                'Fields Found': len(page_fields),
                'PDF File': f'extracted_pdfs/page_{page}.pdf',
                'Key Fields Found': ', '.join(key_fields_found) if key_fields_found else 'None'
            })
        
        if page_data:
            pd.DataFrame(page_data).to_excel(writer, sheet_name='Page Summary', index=False)

def download_pdfs_with_excel(request):
    """
    Creates a ZIP file containing:
    1. Original PDF file
    2. All extracted PDF pages
    3. Detailed Excel file with extraction data
    4. README file explaining the contents
    """
    pdf_id = request.GET.get('pdf_id')
    source_pdf = request.GET.get('source')
    
    try:
        with tempfile.TemporaryDirectory() as temp_dir:
            if pdf_id or source_pdf:
                try:
                    # Get the PDF file
                    if pdf_id:
                        pdf = UploadedPDF.objects.get(id=pdf_id)
                    else:
                        pdf_filename = os.path.basename(source_pdf)
                        pdf = UploadedPDF.objects.filter(file__contains=pdf_filename).first()
                        if not pdf:
                            messages.warning(request, f"No PDF found with filename: {pdf_filename}")
                            return redirect("dashboard")
                    
                    # Get extracted data
                    extracted_data = ExtractedData.objects.filter(pdf=pdf).order_by('field_key')
                    if not extracted_data.exists():
                        messages.warning(request, "No extracted data found for this PDF")
                        return redirect("dashboard")
                    
                    # Set up directory structure
                    pdf_dir = os.path.join(temp_dir, 'package')
                    orig_dir = os.path.join(pdf_dir, 'original')
                    extracted_dir = os.path.join(pdf_dir, 'extracted_pdfs')
                    os.makedirs(orig_dir, exist_ok=True)
                    os.makedirs(extracted_dir, exist_ok=True)
                    
                    # Copy original PDF
                    pdf_filename = os.path.basename(pdf.file.name)
                    orig_pdf_path = os.path.join(orig_dir, pdf_filename)
                    if os.path.exists(pdf.file.path):
                        shutil.copy2(pdf.file.path, orig_pdf_path)
                    
                    # Copy extracted PDFs
                    base_extracted_dir = os.path.join(settings.MEDIA_ROOT, 'extracted')
                    pdf_name_without_ext = os.path.splitext(pdf_filename)[0]
                    extracted_files = []
                    
                    if os.path.exists(base_extracted_dir):
                        for root, _, files in os.walk(base_extracted_dir):
                            for file in files:
                                if file.startswith(pdf_name_without_ext) and file.endswith('.pdf'):
                                    src_path = os.path.join(root, file)
                                    dest_path = os.path.join(extracted_dir, file)
                                    shutil.copy2(src_path, dest_path)
                                    extracted_files.append(file)
                    
                    # Create Excel file
                    excel_path = os.path.join(pdf_dir, 'extraction_summary.xlsx')
                    create_extraction_excel(excel_path, pdf, extracted_data)
                    
                    # Create ZIP file
                    zip_filename = f"{pdf_name_without_ext}_extraction_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
                    zip_buffer = io.BytesIO()
                    
                    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zipf:
                        # Add all files maintaining directory structure
                        for root, _, files in os.walk(pdf_dir):
                            for file in files:
                                file_path = os.path.join(root, file)
                                arcname = os.path.relpath(file_path, pdf_dir)
                                zipf.write(file_path, arcname=arcname)
                        
                        # Add README
                        readme_content = f"""Extraction Summary

PDF: {pdf_filename}
Vendor: {pdf.vendor.name}
Uploaded: {pdf.uploaded_at.strftime('%Y-%m-%d %H:%M:%S')}
Extracted Fields: {extracted_data.count()}

Directory Structure:
- original/           : Original uploaded PDF
- extracted_pdfs/     : Individual extracted pages
- extraction_summary.xlsx : Detailed Excel file with:
  * Summary          : Overview and statistics
  * Extracted Data   : All extracted fields
  * Key Fields       : Certificate data (PLATE_NO, HEAT_NO, etc.)
  * Page Summary     : Page-by-page breakdown

Extracted Files:
{chr(10).join(f"- {file}" for file in extracted_files)}
"""
                        zipf.writestr("README.txt", readme_content)
                    
                except UploadedPDF.DoesNotExist:
                    messages.error(request, "PDF file not found")
                    return redirect("dashboard")
                    
            else:
                # Get all PDFs that have extracted data
                pdfs_with_data = UploadedPDF.objects.filter(extracted_data__isnull=False).distinct()
                
                if not pdfs_with_data.exists():
                    messages.warning(request, "No PDFs with extracted data found")
                    return redirect("dashboard")
                
                # Set up base directory for package
                package_dir = os.path.join(temp_dir, 'package')
                os.makedirs(package_dir, exist_ok=True)
                
                # Process PDFs and create summary
                all_data = []
                for pdf in pdfs_with_data:
                    # Create individual PDF directory
                    pdf_dir = os.path.join(package_dir, pdf_name_without_ext)
                    orig_dir = os.path.join(pdf_dir, 'original')
                    extracted_dir = os.path.join(pdf_dir, 'extracted_pdfs')
                    os.makedirs(orig_dir, exist_ok=True)
                    os.makedirs(extracted_dir, exist_ok=True)
                    
                    # Copy original PDF
                    if os.path.exists(pdf.file.path):
                        shutil.copy2(pdf.file.path, os.path.join(orig_dir, pdf_filename))
                    
                    # Find and copy extracted PDFs
                    base_extracted_dir = os.path.join(settings.MEDIA_ROOT, 'extracted')
                    if os.path.exists(base_extracted_dir):
                        extracted_count = 0
                        for root, _, files in os.walk(base_extracted_dir):
                            for file in files:
                                if file.startswith(pdf_name_without_ext) and file.endswith('.pdf'):
                                    src_path = os.path.join(root, file)
                                    dest_path = os.path.join(extracted_dir, file)
                                    shutil.copy2(src_path, dest_path)
                                    extracted_count += 1
                    
                    # Add to summary data
                    extracted_data = ExtractedData.objects.filter(pdf=pdf).order_by('field_key')
                    key_data = {
                        field: next((item.field_value for item in extracted_data if item.field_key == field), '')
                        for field in ['PLATE_NO', 'HEAT_NO', 'TEST_CERT_NO']
                    }
                    
                    all_data.append({
                        'PDF File': os.path.basename(pdf.file.name),
                        'Vendor': pdf.vendor.name,
                        'PLATE_NO': key_data['PLATE_NO'],
                        'HEAT_NO': key_data['HEAT_NO'],
                        'TEST_CERT_NO': key_data['TEST_CERT_NO'],
                        'Uploaded At': pdf.uploaded_at.strftime("%Y-%m-%d %H:%M:%S"),
                        'Fields Found': extracted_data.count(),
                        'Extracted Pages': extracted_count
                    })
                
                # Create Excel summary
                excel_path = os.path.join(package_dir, 'extraction_summary.xlsx')
                df = pd.DataFrame(all_data)
                df.to_excel(excel_path, index=False)
                
                # Create ZIP file
                zip_filename = f"all_extractions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
                zip_buffer = io.BytesIO()
                
                with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    # Add all files maintaining directory structure
                    for root, _, files in os.walk(package_dir):
                        for file in files:
                            file_path = os.path.join(root, file)
                            arcname = os.path.relpath(file_path, package_dir)
                            zipf.write(file_path, arcname=arcname)
                    
                    # Add README
                    readme_content = """Extraction Summary

This archive contains:
1. Original PDFs and their extracted pages
2. Excel summary of all extracted data

Directory Structure:
- extraction_summary.xlsx : Master Excel file with all extracted data
- <pdf_name>/
  - original/           : Contains the original uploaded PDF
  - extracted_pdfs/     : Contains individual extracted PDFs

Summary:
"""
                    for item in all_data:
                        readme_content += f"\nPDF: {item['PDF File']}\n"
                        readme_content += f"- Vendor: {item['Vendor']}\n"
                        readme_content += f"- Uploaded: {item['Uploaded At']}\n"
                        readme_content += f"- Fields Found: {item['Fields Found']}\n"
                        readme_content += f"- Extracted Pages: {item['Extracted Pages']}\n"
                    
                    readme_content += f"\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
                    zipf.writestr("README.txt", readme_content)
            
            # Reset buffer position and return response
            zip_buffer.seek(0)
            response = FileResponse(
                zip_buffer,
                as_attachment=True,
                filename=zip_filename
            )
            return response
            
    except Exception as e:
        logger.error(f"Error creating ZIP archive: {str(e)}", exc_info=True)
        messages.error(request, "Could not create ZIP archive of PDFs and Excel data")
        return redirect("dashboard")
